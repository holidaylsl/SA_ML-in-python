{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbbfa028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4951e",
   "metadata": {},
   "source": [
    "### training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d0e1d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train\\\\apple_1.jpg',\n",
       " 'train\\\\apple_10.jpg',\n",
       " 'train\\\\apple_11.jpg',\n",
       " 'train\\\\apple_12.jpg',\n",
       " 'train\\\\apple_13.jpg',\n",
       " 'train\\\\apple_14.jpg',\n",
       " 'train\\\\apple_15.jpg',\n",
       " 'train\\\\apple_16.jpg',\n",
       " 'train\\\\apple_17.jpg',\n",
       " 'train\\\\apple_18.jpg',\n",
       " 'train\\\\apple_19.jpg',\n",
       " 'train\\\\apple_2.jpg',\n",
       " 'train\\\\apple_20.jpg',\n",
       " 'train\\\\apple_21.jpg',\n",
       " 'train\\\\apple_22.jpg',\n",
       " 'train\\\\apple_23.jpg',\n",
       " 'train\\\\apple_24.jpg',\n",
       " 'train\\\\apple_25.jpg',\n",
       " 'train\\\\apple_26.jpg',\n",
       " 'train\\\\apple_27.jpg',\n",
       " 'train\\\\apple_28.jpg',\n",
       " 'train\\\\apple_29.jpg',\n",
       " 'train\\\\apple_3.jpg',\n",
       " 'train\\\\apple_30.jpg',\n",
       " 'train\\\\apple_31.jpg',\n",
       " 'train\\\\apple_32.jpg',\n",
       " 'train\\\\apple_33.jpg',\n",
       " 'train\\\\apple_34.jpg',\n",
       " 'train\\\\apple_35.jpg',\n",
       " 'train\\\\apple_36.jpg',\n",
       " 'train\\\\apple_37.jpg',\n",
       " 'train\\\\apple_38.jpg',\n",
       " 'train\\\\apple_39.jpg',\n",
       " 'train\\\\apple_4.jpg',\n",
       " 'train\\\\apple_40.jpg',\n",
       " 'train\\\\apple_41.jpg',\n",
       " 'train\\\\apple_42.jpg',\n",
       " 'train\\\\apple_43.jpg',\n",
       " 'train\\\\apple_44.jpg',\n",
       " 'train\\\\apple_45.jpg',\n",
       " 'train\\\\apple_46.jpg',\n",
       " 'train\\\\apple_47.jpg',\n",
       " 'train\\\\apple_48.jpg',\n",
       " 'train\\\\apple_49.jpg',\n",
       " 'train\\\\apple_5.jpg',\n",
       " 'train\\\\apple_50.jpg',\n",
       " 'train\\\\apple_51.jpg',\n",
       " 'train\\\\apple_52.jpg',\n",
       " 'train\\\\apple_53.jpg',\n",
       " 'train\\\\apple_54.jpg',\n",
       " 'train\\\\apple_55.jpg',\n",
       " 'train\\\\apple_56.jpg',\n",
       " 'train\\\\apple_57.jpg',\n",
       " 'train\\\\apple_58.jpg',\n",
       " 'train\\\\apple_59.jpg',\n",
       " 'train\\\\apple_6.jpg',\n",
       " 'train\\\\apple_60.jpg',\n",
       " 'train\\\\apple_61.jpg',\n",
       " 'train\\\\apple_62.jpg',\n",
       " 'train\\\\apple_63.jpg',\n",
       " 'train\\\\apple_64.jpg',\n",
       " 'train\\\\apple_65.jpg',\n",
       " 'train\\\\apple_66.jpg',\n",
       " 'train\\\\apple_67.jpg',\n",
       " 'train\\\\apple_68.jpg',\n",
       " 'train\\\\apple_69.jpg',\n",
       " 'train\\\\apple_7.jpg',\n",
       " 'train\\\\apple_70.jpg',\n",
       " 'train\\\\apple_71.jpg',\n",
       " 'train\\\\apple_72.jpg',\n",
       " 'train\\\\apple_73.jpg',\n",
       " 'train\\\\apple_74.jpg',\n",
       " 'train\\\\apple_75.jpg',\n",
       " 'train\\\\apple_76.jpg',\n",
       " 'train\\\\apple_8.jpg',\n",
       " 'train\\\\apple_9.jpg',\n",
       " 'train\\\\banana_1.jpg',\n",
       " 'train\\\\banana_10.jpg',\n",
       " 'train\\\\banana_11.jpg',\n",
       " 'train\\\\banana_12.jpg',\n",
       " 'train\\\\banana_13.jpg',\n",
       " 'train\\\\banana_14.jpg',\n",
       " 'train\\\\banana_15.jpg',\n",
       " 'train\\\\banana_16.jpg',\n",
       " 'train\\\\banana_17.jpg',\n",
       " 'train\\\\banana_18.jpg',\n",
       " 'train\\\\banana_19.jpg',\n",
       " 'train\\\\banana_2.jpg',\n",
       " 'train\\\\banana_20.jpg',\n",
       " 'train\\\\banana_21.jpg',\n",
       " 'train\\\\banana_22.jpg',\n",
       " 'train\\\\banana_23.jpg',\n",
       " 'train\\\\banana_24.jpg',\n",
       " 'train\\\\banana_25.jpg',\n",
       " 'train\\\\banana_26.jpg',\n",
       " 'train\\\\banana_27.jpg',\n",
       " 'train\\\\banana_28.jpg',\n",
       " 'train\\\\banana_29.jpg',\n",
       " 'train\\\\banana_3.jpg',\n",
       " 'train\\\\banana_30.jpg',\n",
       " 'train\\\\banana_31.jpg',\n",
       " 'train\\\\banana_32.jpg',\n",
       " 'train\\\\banana_33.jpg',\n",
       " 'train\\\\banana_34.jpg',\n",
       " 'train\\\\banana_35.jpg',\n",
       " 'train\\\\banana_36.jpg',\n",
       " 'train\\\\banana_37.jpg',\n",
       " 'train\\\\banana_38.jpg',\n",
       " 'train\\\\banana_39.jpg',\n",
       " 'train\\\\banana_4.jpg',\n",
       " 'train\\\\banana_40.jpg',\n",
       " 'train\\\\banana_41.jpg',\n",
       " 'train\\\\banana_42.jpg',\n",
       " 'train\\\\banana_43.jpg',\n",
       " 'train\\\\banana_44.jpg',\n",
       " 'train\\\\banana_45.jpg',\n",
       " 'train\\\\banana_46.jpg',\n",
       " 'train\\\\banana_47.jpg',\n",
       " 'train\\\\banana_48.jpg',\n",
       " 'train\\\\banana_49.jpg',\n",
       " 'train\\\\banana_5.jpg',\n",
       " 'train\\\\banana_50.jpg',\n",
       " 'train\\\\banana_51.jpg',\n",
       " 'train\\\\banana_52.jpg',\n",
       " 'train\\\\banana_53.jpg',\n",
       " 'train\\\\banana_54.jpg',\n",
       " 'train\\\\banana_55.jpg',\n",
       " 'train\\\\banana_56.jpg',\n",
       " 'train\\\\banana_57.jpg',\n",
       " 'train\\\\banana_58.jpg',\n",
       " 'train\\\\banana_59.jpg',\n",
       " 'train\\\\banana_6.jpg',\n",
       " 'train\\\\banana_60.jpg',\n",
       " 'train\\\\banana_61.jpg',\n",
       " 'train\\\\banana_62.jpg',\n",
       " 'train\\\\banana_63.jpg',\n",
       " 'train\\\\banana_64.jpg',\n",
       " 'train\\\\banana_65.jpg',\n",
       " 'train\\\\banana_66.jpg',\n",
       " 'train\\\\banana_67.jpg',\n",
       " 'train\\\\banana_68.jpg',\n",
       " 'train\\\\banana_69.jpg',\n",
       " 'train\\\\banana_7.jpg',\n",
       " 'train\\\\banana_70.jpg',\n",
       " 'train\\\\banana_71.jpg',\n",
       " 'train\\\\banana_72.jpg',\n",
       " 'train\\\\banana_73.jpg',\n",
       " 'train\\\\banana_74.jpg',\n",
       " 'train\\\\banana_75.jpg',\n",
       " 'train\\\\banana_76.jpg',\n",
       " 'train\\\\banana_8.jpg',\n",
       " 'train\\\\banana_9.jpg',\n",
       " 'train\\\\mixed_1.jpg',\n",
       " 'train\\\\mixed_10.jpg',\n",
       " 'train\\\\mixed_11.jpg',\n",
       " 'train\\\\mixed_12.jpg',\n",
       " 'train\\\\mixed_13.jpg',\n",
       " 'train\\\\mixed_14.jpg',\n",
       " 'train\\\\mixed_15.jpg',\n",
       " 'train\\\\mixed_16.jpg',\n",
       " 'train\\\\mixed_17.jpg',\n",
       " 'train\\\\mixed_18.jpg',\n",
       " 'train\\\\mixed_19.jpg',\n",
       " 'train\\\\mixed_2.jpg',\n",
       " 'train\\\\mixed_20.jpg',\n",
       " 'train\\\\mixed_3.jpg',\n",
       " 'train\\\\mixed_4.jpg',\n",
       " 'train\\\\mixed_5.jpg',\n",
       " 'train\\\\mixed_6.jpg',\n",
       " 'train\\\\mixed_7.jpg',\n",
       " 'train\\\\mixed_8.jpg',\n",
       " 'train\\\\mixed_9.jpg',\n",
       " 'train\\\\orange_1.jpg',\n",
       " 'train\\\\orange_10.jpg',\n",
       " 'train\\\\orange_11.jpg',\n",
       " 'train\\\\orange_12.jpg',\n",
       " 'train\\\\orange_13.jpg',\n",
       " 'train\\\\orange_14.jpg',\n",
       " 'train\\\\orange_15.jpg',\n",
       " 'train\\\\orange_16.jpg',\n",
       " 'train\\\\orange_17.jpg',\n",
       " 'train\\\\orange_18.jpg',\n",
       " 'train\\\\orange_19.jpg',\n",
       " 'train\\\\orange_2.jpg',\n",
       " 'train\\\\orange_20.jpg',\n",
       " 'train\\\\orange_21.jpg',\n",
       " 'train\\\\orange_22.jpg',\n",
       " 'train\\\\orange_23.jpg',\n",
       " 'train\\\\orange_24.jpg',\n",
       " 'train\\\\orange_25.jpg',\n",
       " 'train\\\\orange_26.jpg',\n",
       " 'train\\\\orange_27.jpg',\n",
       " 'train\\\\orange_28.jpg',\n",
       " 'train\\\\orange_29.jpg',\n",
       " 'train\\\\orange_3.jpg',\n",
       " 'train\\\\orange_30.jpg',\n",
       " 'train\\\\orange_31.jpg',\n",
       " 'train\\\\orange_32.jpg',\n",
       " 'train\\\\orange_33.jpg',\n",
       " 'train\\\\orange_34.jpg',\n",
       " 'train\\\\orange_35.jpg',\n",
       " 'train\\\\orange_36.jpg',\n",
       " 'train\\\\orange_37.jpg',\n",
       " 'train\\\\orange_38.jpg',\n",
       " 'train\\\\orange_39.jpg',\n",
       " 'train\\\\orange_4.jpg',\n",
       " 'train\\\\orange_40.jpg',\n",
       " 'train\\\\orange_41.jpg',\n",
       " 'train\\\\orange_42.jpg',\n",
       " 'train\\\\orange_43.jpg',\n",
       " 'train\\\\orange_44.jpg',\n",
       " 'train\\\\orange_45.jpg',\n",
       " 'train\\\\orange_46.jpg',\n",
       " 'train\\\\orange_47.jpg',\n",
       " 'train\\\\orange_48.jpg',\n",
       " 'train\\\\orange_49.jpg',\n",
       " 'train\\\\orange_5.jpg',\n",
       " 'train\\\\orange_50.jpg',\n",
       " 'train\\\\orange_51.jpg',\n",
       " 'train\\\\orange_52.jpg',\n",
       " 'train\\\\orange_53.jpg',\n",
       " 'train\\\\orange_54.jpg',\n",
       " 'train\\\\orange_55.jpg',\n",
       " 'train\\\\orange_56.jpg',\n",
       " 'train\\\\orange_57.jpg',\n",
       " 'train\\\\orange_58.jpg',\n",
       " 'train\\\\orange_59.jpg',\n",
       " 'train\\\\orange_6.jpg',\n",
       " 'train\\\\orange_60.jpg',\n",
       " 'train\\\\orange_61.jpg',\n",
       " 'train\\\\orange_62.jpg',\n",
       " 'train\\\\orange_63.jpg',\n",
       " 'train\\\\orange_64.jpg',\n",
       " 'train\\\\orange_65.jpg',\n",
       " 'train\\\\orange_66.jpg',\n",
       " 'train\\\\orange_67.jpg',\n",
       " 'train\\\\orange_68.jpg',\n",
       " 'train\\\\orange_69.jpg',\n",
       " 'train\\\\orange_7.jpg',\n",
       " 'train\\\\orange_70.jpg',\n",
       " 'train\\\\orange_71.jpg',\n",
       " 'train\\\\orange_72.jpg',\n",
       " 'train\\\\orange_73.jpg',\n",
       " 'train\\\\orange_74.jpg',\n",
       " 'train\\\\orange_75.jpg',\n",
       " 'train\\\\orange_76.jpg',\n",
       " 'train\\\\orange_8.jpg',\n",
       " 'train\\\\orange_9.jpg']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_all = glob.glob(r'train/*.jpg')\n",
    "img_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18f74487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(img_all).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "272f56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15872, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(248, 64, 64, 3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the feature\n",
    "x_train = None\n",
    "for i in img_all:\n",
    "    img = Image.open(i).convert('RGB')\n",
    "    img_resize = img.resize((64,64))\n",
    "    if x_train is None:\n",
    "        x_train = img_resize\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train, img_resize))\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(-1,64,64,3)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "15c10cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label: 1hot-encode: [1,0,0]-apple  [0,1,0]-banana  [0,0,1]-orange  [1,1,1]-mixed\n",
    "y = []\n",
    "for i in img_all:\n",
    "    if 'apple' in i:\n",
    "        y.append([1,0,0])\n",
    "    if 'banana' in i:\n",
    "        y.append([0,1,0])\n",
    "    if 'orange' in i:\n",
    "        y.append([0,0,1])\n",
    "    if 'mixed' in i:\n",
    "        y.append([1,1,1])\n",
    "y_train = np.array(y)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9514f",
   "metadata": {},
   "source": [
    "### create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16dec1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.Sequential()\n",
    "\n",
    "model_1.add(tf.keras.layers.Conv2D(filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "    input_shape=(64, 64, 3)))\n",
    "model_1.add(tf.keras.layers.Conv2D(filters=32,\n",
    "    kernel_size=(5, 5),\n",
    "    activation='relu',\n",
    "    input_shape=(64, 64, 3)))\n",
    "model_1.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "model_1.add(tf.keras.layers.Flatten())\n",
    "model_1.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "model_1.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model_1.add(tf.keras.layers.Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "728a0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "57fb3fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 1.6866 - accuracy: 0.3710\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 1s 161ms/step - loss: 1.3210 - accuracy: 0.3911\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 2.7259 - accuracy: 0.3427\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 3.4667 - accuracy: 0.3710\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 5.0687 - accuracy: 0.3871\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 12.4879 - accuracy: 0.3750\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 29.7899 - accuracy: 0.3468\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 1s 175ms/step - loss: 76.2492 - accuracy: 0.3387\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 124.5259 - accuracy: 0.3710\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 206.2210 - accuracy: 0.3226\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 262.0712 - accuracy: 0.3992\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 324.1296 - accuracy: 0.3185\n",
      "Epoch 13/15\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 374.1385 - accuracy: 0.3347\n",
      "Epoch 14/15\n",
      "8/8 [==============================] - 1s 182ms/step - loss: 460.0993 - accuracy: 0.3548\n",
      "Epoch 15/15\n",
      "8/8 [==============================] - 1s 175ms/step - loss: 496.5639 - accuracy: 0.3508\n"
     ]
    }
   ],
   "source": [
    "hist = model_1.fit(x=x_train/255, y=y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b30c98",
   "metadata": {},
   "source": [
    "### change the 1hot-encoding pattern, and train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9d3869b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label: 1hot-encode: [1,0,0,0]-apple  [0,1,0,0]-banana  [0,0,1,0]-orange  [0,0,0,1]-mixed\n",
    "y = []\n",
    "for i in img_all:\n",
    "    if 'apple' in i:\n",
    "        y.append([1,0,0,0])\n",
    "    if 'banana' in i:\n",
    "        y.append([0,1,0,0])\n",
    "    if 'orange' in i:\n",
    "        y.append([0,0,1,0])\n",
    "    if 'mixed' in i:\n",
    "        y.append([0,0,0,1])\n",
    "y_train = np.array(y)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "69660247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential()\n",
    "\n",
    "model_2.add(tf.keras.layers.Conv2D(filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "    input_shape=(64, 64, 3)))\n",
    "model_2.add(tf.keras.layers.Conv2D(filters=32,\n",
    "    kernel_size=(5, 5),\n",
    "    activation='relu',\n",
    "    input_shape=(64, 64, 3)))\n",
    "model_2.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "model_2.add(tf.keras.layers.Flatten())\n",
    "model_2.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "model_2.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model_2.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "225b04c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 1.4714 - accuracy: 0.4073\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 1s 175ms/step - loss: 1.1035 - accuracy: 0.5645\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 1s 160ms/step - loss: 0.9055 - accuracy: 0.6290\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 0.8205 - accuracy: 0.6935\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 1s 160ms/step - loss: 0.5942 - accuracy: 0.7621\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.5516 - accuracy: 0.8024\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 0.4314 - accuracy: 0.8306\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 1s 160ms/step - loss: 0.4065 - accuracy: 0.8427\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 1s 163ms/step - loss: 0.3443 - accuracy: 0.8952\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 1s 160ms/step - loss: 0.2862 - accuracy: 0.9073\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 0.3108 - accuracy: 0.8871\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 0.2398 - accuracy: 0.9395\n",
      "Epoch 13/15\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 0.2222 - accuracy: 0.9234\n",
      "Epoch 14/15\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 0.2315 - accuracy: 0.8992\n",
      "Epoch 15/15\n",
      "8/8 [==============================] - 1s 165ms/step - loss: 0.1921 - accuracy: 0.9274\n"
     ]
    }
   ],
   "source": [
    "#x_train does not change\n",
    "hist = model_2.fit(x=x_train/255, y=y_train, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "86961494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31744, 128, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(248, 128, 128, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy improved a lot after adjusting the label encoding. \n",
    "# now resize the images to larger(128x128) to see the effect:\n",
    "x_train = None\n",
    "for i in img_all:\n",
    "    img = Image.open(i).convert('RGB')\n",
    "    img_resize = img.resize((128,128))\n",
    "    if x_train is None:\n",
    "        x_train = img_resize\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train, img_resize))\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(-1,128,128,3)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68743af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = tf.keras.Sequential()\n",
    "\n",
    "model_3.add(tf.keras.layers.Conv2D(filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    activation='relu',\n",
    "    input_shape=(128, 128, 3)))\n",
    "model_3.add(tf.keras.layers.Conv2D(filters=32,\n",
    "    kernel_size=(5, 5),\n",
    "    activation='relu',\n",
    "    input_shape=(128, 128, 3)))\n",
    "model_3.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_3.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "model_3.add(tf.keras.layers.Flatten())\n",
    "model_3.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "model_3.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model_3.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3600e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 6s 786ms/step - loss: 2.2619 - accuracy: 0.3548\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 6s 773ms/step - loss: 0.9597 - accuracy: 0.6331\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 7s 818ms/step - loss: 0.6265 - accuracy: 0.7702\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 6s 768ms/step - loss: 0.4816 - accuracy: 0.8185\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 6s 762ms/step - loss: 0.2686 - accuracy: 0.9315\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 7s 822ms/step - loss: 0.2480 - accuracy: 0.9073\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 6s 745ms/step - loss: 0.1775 - accuracy: 0.9476\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 6s 750ms/step - loss: 0.0861 - accuracy: 0.9839\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 6s 760ms/step - loss: 0.1060 - accuracy: 0.9758\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 6s 764ms/step - loss: 0.0824 - accuracy: 0.9758\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 6s 769ms/step - loss: 0.0973 - accuracy: 0.9718\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 6s 753ms/step - loss: 0.0741 - accuracy: 0.9879\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 6s 761ms/step - loss: 0.0972 - accuracy: 0.9677\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 6s 756ms/step - loss: 0.0667 - accuracy: 0.9798\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 6s 759ms/step - loss: 0.0567 - accuracy: 0.9839\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 6s 765ms/step - loss: 0.0220 - accuracy: 0.9960\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 6s 760ms/step - loss: 0.0360 - accuracy: 0.9839\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 6s 777ms/step - loss: 0.0234 - accuracy: 0.9960\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 6s 759ms/step - loss: 0.0704 - accuracy: 0.9839\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 6s 762ms/step - loss: 0.0136 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# y_train does not change\n",
    "hist = model_3.fit(x=x_train/255, y=y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0c73432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slight improvement. ready for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549abb6e",
   "metadata": {},
   "source": [
    "### prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6760ce24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test\\\\apple_77.jpg',\n",
       " 'test\\\\apple_78.jpg',\n",
       " 'test\\\\apple_79.jpg',\n",
       " 'test\\\\apple_80.jpg',\n",
       " 'test\\\\apple_81.jpg',\n",
       " 'test\\\\apple_82.jpg',\n",
       " 'test\\\\apple_83.jpg',\n",
       " 'test\\\\apple_84.jpg',\n",
       " 'test\\\\apple_85.jpg',\n",
       " 'test\\\\apple_86.jpg',\n",
       " 'test\\\\apple_87.jpg',\n",
       " 'test\\\\apple_88.jpg',\n",
       " 'test\\\\apple_89.jpg',\n",
       " 'test\\\\apple_90.jpg',\n",
       " 'test\\\\apple_91.jpg',\n",
       " 'test\\\\apple_92.jpg',\n",
       " 'test\\\\apple_93.jpg',\n",
       " 'test\\\\apple_94.jpg',\n",
       " 'test\\\\apple_95.jpg',\n",
       " 'test\\\\banana_77.jpg',\n",
       " 'test\\\\banana_78.jpg',\n",
       " 'test\\\\banana_79.jpg',\n",
       " 'test\\\\banana_80.jpg',\n",
       " 'test\\\\banana_81.jpg',\n",
       " 'test\\\\banana_82.jpg',\n",
       " 'test\\\\banana_83.jpg',\n",
       " 'test\\\\banana_84.jpg',\n",
       " 'test\\\\banana_85.jpg',\n",
       " 'test\\\\banana_86.jpg',\n",
       " 'test\\\\banana_87.jpg',\n",
       " 'test\\\\banana_88.jpg',\n",
       " 'test\\\\banana_89.jpg',\n",
       " 'test\\\\banana_90.jpg',\n",
       " 'test\\\\banana_91.jpg',\n",
       " 'test\\\\banana_92.jpg',\n",
       " 'test\\\\banana_93.jpg',\n",
       " 'test\\\\banana_94.jpg',\n",
       " 'test\\\\mixed_21.jpg',\n",
       " 'test\\\\mixed_22.jpg',\n",
       " 'test\\\\mixed_23.jpg',\n",
       " 'test\\\\mixed_24.jpg',\n",
       " 'test\\\\mixed_25.jpg',\n",
       " 'test\\\\orange_77.jpg',\n",
       " 'test\\\\orange_78.jpg',\n",
       " 'test\\\\orange_79.jpg',\n",
       " 'test\\\\orange_80.jpg',\n",
       " 'test\\\\orange_81.jpg',\n",
       " 'test\\\\orange_82.jpg',\n",
       " 'test\\\\orange_83.jpg',\n",
       " 'test\\\\orange_84.jpg',\n",
       " 'test\\\\orange_85.jpg',\n",
       " 'test\\\\orange_86.jpg',\n",
       " 'test\\\\orange_87.jpg',\n",
       " 'test\\\\orange_89.jpg',\n",
       " 'test\\\\orange_90.jpg',\n",
       " 'test\\\\orange_91.jpg',\n",
       " 'test\\\\orange_92.jpg',\n",
       " 'test\\\\orange_93.jpg',\n",
       " 'test\\\\orange_94.jpg',\n",
       " 'test\\\\orange_95.jpg']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test = glob.glob(r'test/*.jpg')\n",
    "img_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a326f438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_truth = []\n",
    "for i in img_test:\n",
    "    if 'apple' in i:\n",
    "        y_truth.append([1,0,0,0])\n",
    "    if 'banana' in i:\n",
    "        y_truth.append([0,1,0,0])\n",
    "    if 'orange' in i:\n",
    "        y_truth.append([0,0,1,0])\n",
    "    if 'mixed' in i:\n",
    "        y_truth.append([0,0,0,1])\n",
    "y_truth = np.array(y_truth)\n",
    "y_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc655d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 64, 64, 3) \n",
      " (60, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "x_test_64 = None\n",
    "x_test_128 = None\n",
    "for i in img_test:\n",
    "    im = Image.open(i).convert('RGB')\n",
    "    im_64 = im.resize((64,64))\n",
    "    im_128 = im.resize((128,128))\n",
    "    if x_test_64 is None:\n",
    "        x_test_64 = im_64\n",
    "        x_test_128 = im_128\n",
    "    else:\n",
    "        x_test_64 = np.concatenate((x_test_64, im_64))\n",
    "        x_test_128 = np.concatenate((x_test_128, im_128))\n",
    "x_test_64 = x_test_64.reshape((-1,64,64,3))\n",
    "x_test_128 = x_test_128.reshape((-1,128,128,3))\n",
    "print(x_test_64.shape, '\\n', x_test_128.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0f95a",
   "metadata": {},
   "source": [
    "### test the models and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "db5200db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 12ms/step - loss: 154.8541 - accuracy: 0.8500\n",
      "loss_64 = 154.85411071777344\n",
      "accuracy_64 = 0.8500000238418579\n"
     ]
    }
   ],
   "source": [
    "loss_64, accuracy_64 = model_2.evaluate(x=x_test_64, y=y_truth)\n",
    "print('loss_64 =', loss_64)\n",
    "print('accuracy_64 =', accuracy_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb6552e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 51ms/step - loss: 193.3968 - accuracy: 0.8833\n",
      "loss_128 = 193.39683532714844\n",
      "accuracy_128 = 0.8833333253860474\n"
     ]
    }
   ],
   "source": [
    "loss_128, accuracy_128 = model_3.evaluate(x=x_test_128, y=y_truth)\n",
    "print('loss_128 =', loss_128)\n",
    "print('accuracy_128 =', accuracy_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8d16081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [1 0 0 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [1 0 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [1. 0. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 1 0 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 0 1]\n",
      "predict: [0. 0. 0. 1.]    truth: [0 0 0 1]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 0 0 1]\n",
      "predict: [0. 1. 0. 0.]    truth: [0 0 0 1]\n",
      "predict: [1. 0. 0. 0.]    truth: [0 0 0 1]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "predict: [0. 0. 1. 0.]    truth: [0 0 1 0]\n",
      "correct =  53     wrong =  7\n",
      "accuracy =  0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_3.predict(x_test_128)\n",
    "correct = 0\n",
    "wrong = 0\n",
    "for i in range(len(y_pred)):\n",
    "    print('predict:', y_pred[i], '   truth:', y_truth[i])\n",
    "    if (y_pred[i].tolist() == y_truth[i].tolist()):\n",
    "        correct+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "print('correct = ', correct, '   ', 'wrong = ', wrong)\n",
    "print('accuracy = ',correct/(correct+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964f6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
